{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import tweepy\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "import GetOldTweets3 as got\n",
    "\n",
    "consumer_key=\"hME3af4IKB8PG1bACoFWIOsb7\"\n",
    "consumer_secret=\"5NVgAZqgezRM1fApMwuv4bF2lagForppzEClipxYymL4EnWztb\"\n",
    "access_token=\"2257981634-ERJNYPvjln32aQlNygNAyUNImFn2TCzE1VBYIkG\"\n",
    "access_token_secret=\"hqmDnz3x0iQoEbmAJvq6AAN1rFHbuFetoTs5hPf20Gqdq\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciton to get tweet ids already in file\n",
    "def get_ids(file_name):\n",
    "    result=[]\n",
    "    # if the file exists\n",
    "    try:\n",
    "        with open(file_name, 'r') as fp:\n",
    "            line=fp.readline()\n",
    "            # get all tweet ids in the file\n",
    "            while line:\n",
    "                tjson=json.loads(line)\n",
    "                result.append(tjson[\"id\"])\n",
    "                line=fp.readline()\n",
    "        return result\n",
    "    except IOError:\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "# function from tutorial\n",
    "def crawl_tweet(file_name=\"raw_tweet.json\", search_key=\"Twitter\", max_tweet=1000, max_id=-1):\n",
    "    # call get_ids on starting file\n",
    "    tweet_ids = get_ids(file_name)\n",
    "    tweet_count = 0\n",
    "    # append to file instead of writing so we don't wipe previous data\n",
    "    with open(file_name, 'a') as fp:\n",
    "        fp.write(\"\")\n",
    "    while tweet_count<max_tweet:\n",
    "        try:\n",
    "            if max_id<=0:\n",
    "                new_tweets=api.search(q=search_key, count=100)\n",
    "            else:\n",
    "                new_tweets=api.search(q=search_key, count=100, max_id=str(max_id-1))\n",
    "            if not new_tweets:\n",
    "                print('No more tweets')\n",
    "                break\n",
    "            with open(file_name, 'a') as fp:\n",
    "                for tweet in new_tweets:\n",
    "                    # convert tweet to json\n",
    "                    tjson=tweet._json\n",
    "                    # filter duplicate tweets\n",
    "                    if tjson['id'] not in tweet_ids:\n",
    "                        # add tweet to text file\n",
    "                        fp.write(json.dumps(tjson)+\"\\n\")\n",
    "                        # add tweet id to tweet_ids\n",
    "                        tweet_ids.append(tjson['id'])\n",
    "            max_id=new_tweets[-1].id\n",
    "            tweet_count+=len(new_tweets)\n",
    "            print(f\"Number of tweets: {tweet_count}\")\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"some error: \" + str(e))\n",
    "            break\n",
    "        \n",
    "\n",
    "# modified function from tutorial using getoldtweets3\n",
    "def crawl_old_tweet(file_name=\"raw_tweet.json\", search_key=\"Twitter\", since=\"2020-04-26\", until=\"2020-04-27\",num=1000):\n",
    "    # call get_ids on starting file\n",
    "    tweet_ids = get_ids(file_name)\n",
    "    # append to file instead of writing so we don't wipe previous data\n",
    "    with open(file_name, 'a') as fp:\n",
    "        fp.write(\"\")\n",
    "    try:\n",
    "        tweet_criteria=got.manager.TweetCriteria().setQuerySearch(search_key).setSince(since).setUntil(until).setMaxTweets(num)\n",
    "        new_tweets = got.manager.TweetManager.getTweets(tweet_criteria)\n",
    "        with open(file_name, 'a') as fp:\n",
    "            for tweet in new_tweets:\n",
    "                tweet_info = {\n",
    "                            \"Date\":tweet.date,\n",
    "                            \"id\":tweet.id,\n",
    "                            \"favorite_count\":tweet.favorites, #number of favorites\n",
    "                            \"retweet_count\":tweet.retweets, #number of retweets\n",
    "                            \"text\":tweet.text\n",
    "                            }\n",
    "                # convert tweet to json\n",
    "                tjson=json.loads(json.dumps(tweet_info, default=str))\n",
    "                # filter duplicate tweets\n",
    "                if tjson['id'] not in tweet_ids:\n",
    "                    # add tweet to text file\n",
    "                    fp.write(json.dumps(tjson)+\"\\n\")\n",
    "                    # add tweet id to tweet_ids\n",
    "                    tweet_ids.append(tjson['id'])\n",
    "        tweet_count = len(new_tweets)\n",
    "        print(f\"Number of tweets: {tweet_count}\")\n",
    "    except tweepy.TweepError as e:\n",
    "        print(\"some error: \" + str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read keywords (in this case just climate change)\n",
    "\n",
    "input_keywords=\"input_keywords.txt\"\n",
    "# read keyswords in\n",
    "with open(input_keywords) as fp2:\n",
    "    keywords = fp2.readlines()\n",
    "    \n",
    "# remove newlines\n",
    "for i in range(len(keywords)):\n",
    "    keywords[i] = keywords[i].replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of total days or each months\n",
    "jan = [\"2020-01-0\" + str(x) if x < 10 else \"2020-01-\" + str(x) for x in range(22,32)]\n",
    "feb = [\"2020-02-0\" + str(x) if x < 10 else \"2020-02-\" + str(x) for x in range(1,30)]\n",
    "mar = [\"2020-03-0\" + str(x) if x < 10 else \"2020-03-\" + str(x) for x in range(1,32)]\n",
    "apr = [\"2020-04-0\" + str(x) if x < 10 else \"2020-04-\" + str(x) for x in range(1,4)]\n",
    "\n",
    "days = jan + feb + mar + apr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_day:0\n",
      "02:43:41\n",
      "Number of tweets: 2540\n",
      "02:46:01\n"
     ]
    }
   ],
   "source": [
    "# run the getoldtweets3 for all days\n",
    "cur_day = 0 \n",
    "### Crawl old\n",
    "days = apr\n",
    "# for each keyword\n",
    "for k in keywords:\n",
    "    # for each day \n",
    "    #(using while loop so I can update the starting curent day to contunue where I left off in case of error)\n",
    "    while cur_day < len(days)-1:\n",
    "        # print out curent day value (in case of error)\n",
    "        print(\"cur_day:\" + str(cur_day))\n",
    "        # print start time for analysis of run time (helps when debugging)\n",
    "        print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        # crawl all tweets from input day\n",
    "        crawl_old_tweet(file_name=\"old_tweets.json\", search_key=k, since=days[cur_day], until=days[cur_day+1],num=-1)\n",
    "        # print stop time\n",
    "        print(datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        # wait 2.5 mins to avoid making too many requests resulting in error\n",
    "        time.sleep(150)\n",
    "        # update cur_day counter\n",
    "        cur_day+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Crawl tweets from most recent (using twitter api)\n",
    "\n",
    "# call crawel tweets on each keyword\n",
    "for k in keywords:\n",
    "    crawl_tweet(file_name='tweets.json', search_key=k, max_tweet=18000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time = 13:45:21\n",
      "Number of tweets: 100\n",
      "Number of tweets: 200\n",
      "Number of tweets: 300\n",
      "Number of tweets: 400\n",
      "Number of tweets: 500\n",
      "Number of tweets: 600\n",
      "Number of tweets: 700\n",
      "Number of tweets: 800\n",
      "Number of tweets: 900\n",
      "Number of tweets: 1000\n",
      "Number of tweets: 1100\n",
      "Number of tweets: 1200\n",
      "Number of tweets: 1300\n",
      "Number of tweets: 1400\n",
      "Number of tweets: 1500\n",
      "Number of tweets: 1600\n",
      "Number of tweets: 1700\n",
      "Number of tweets: 1800\n",
      "Number of tweets: 1900\n",
      "Number of tweets: 2000\n",
      "Number of tweets: 2100\n",
      "Number of tweets: 2200\n",
      "Number of tweets: 2300\n",
      "Number of tweets: 2400\n",
      "Number of tweets: 2500\n",
      "Number of tweets: 2600\n",
      "Number of tweets: 2700\n",
      "Number of tweets: 2800\n",
      "Number of tweets: 2900\n",
      "Number of tweets: 3000\n",
      "Number of tweets: 3100\n",
      "Number of tweets: 3200\n",
      "Number of tweets: 3300\n",
      "Number of tweets: 3400\n",
      "Number of tweets: 3500\n",
      "Number of tweets: 3600\n",
      "Number of tweets: 3700\n",
      "Number of tweets: 3800\n",
      "Number of tweets: 3900\n",
      "Number of tweets: 3997\n",
      "Number of tweets: 4097\n",
      "Number of tweets: 4197\n",
      "Number of tweets: 4297\n",
      "Number of tweets: 4397\n",
      "Number of tweets: 4497\n",
      "Number of tweets: 4597\n",
      "Number of tweets: 4697\n",
      "Number of tweets: 4797\n",
      "Number of tweets: 4897\n",
      "Number of tweets: 4997\n",
      "Number of tweets: 5097\n",
      "Number of tweets: 5197\n",
      "Number of tweets: 5297\n",
      "Number of tweets: 5397\n",
      "Number of tweets: 5497\n",
      "Number of tweets: 5597\n",
      "Number of tweets: 5697\n",
      "Number of tweets: 5797\n",
      "Number of tweets: 5897\n",
      "Number of tweets: 5997\n",
      "Number of tweets: 6097\n",
      "Number of tweets: 6197\n",
      "Number of tweets: 6297\n",
      "Number of tweets: 6382\n",
      "Number of tweets: 6475\n",
      "Number of tweets: 6575\n",
      "Number of tweets: 6675\n",
      "Number of tweets: 6775\n",
      "Number of tweets: 6875\n",
      "Number of tweets: 6975\n",
      "Number of tweets: 7075\n",
      "Number of tweets: 7175\n",
      "Number of tweets: 7259\n",
      "Number of tweets: 7359\n",
      "Number of tweets: 7459\n",
      "Number of tweets: 7559\n",
      "Number of tweets: 7659\n",
      "Number of tweets: 7759\n",
      "Number of tweets: 7852\n",
      "Number of tweets: 7952\n",
      "Number of tweets: 8052\n",
      "Number of tweets: 8152\n",
      "Number of tweets: 8175\n",
      "No more tweets\n"
     ]
    }
   ],
   "source": [
    "### crawl tweets from oldest in tweets.json (using twitter api)\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)\n",
    "\n",
    "# call get_ids on starting file\n",
    "tweet_ids = get_ids('tweets.json')\n",
    "last = min(tweet_ids)\n",
    "# call crawel tweets on each keyword\n",
    "for k in keywords:\n",
    "    crawl_tweet(file_name='tweets.json', search_key=k, max_tweet=18000, max_id=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
