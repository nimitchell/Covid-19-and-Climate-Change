{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import tweepy\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "consumer_key=\"hME3af4IKB8PG1bACoFWIOsb7\"\n",
    "consumer_secret=\"5NVgAZqgezRM1fApMwuv4bF2lagForppzEClipxYymL4EnWztb\"\n",
    "access_token=\"2257981634-ERJNYPvjln32aQlNygNAyUNImFn2TCzE1VBYIkG\"\n",
    "access_token_secret=\"hqmDnz3x0iQoEbmAJvq6AAN1rFHbuFetoTs5hPf20Gqdq\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciton to get tweet ids already in file\n",
    "def get_ids(file_name):\n",
    "    result=[]\n",
    "    try:\n",
    "        with open(file_name, 'r') as fp:\n",
    "            line=fp.readline()\n",
    "            while line:\n",
    "                tjson=json.loads(line)\n",
    "                result.append(tjson[\"id\"])\n",
    "                line=fp.readline()\n",
    "        return result\n",
    "    except IOError:\n",
    "        return result\n",
    "\n",
    "# call get_ids on starting file\n",
    "tweet_ids = get_ids('tweets.json')\n",
    "\n",
    "# function from tutorial\n",
    "def crawl_tweet(file_name=\"raw_tweet.json\", search_key=\"Twitter\", max_tweet=1000, max_id=-1):\n",
    "    tweet_count = 0\n",
    "    # append to file instead of writing so we don't wipe previous data\n",
    "    with open(file_name, 'a') as fp:\n",
    "        fp.write(\"\")\n",
    "    while tweet_count<max_tweet:\n",
    "        try:\n",
    "            if max_id<=0:\n",
    "                new_tweets=api.search(q=search_key, count=100)\n",
    "            else:\n",
    "                new_tweets=api.search(q=search_key, count=100, max_id=str(max_id-1))\n",
    "            if not new_tweets:\n",
    "                print('No more tweets')\n",
    "                break\n",
    "            with open(file_name, 'a') as fp:\n",
    "                for tweet in new_tweets:\n",
    "                    # convert tweet to json\n",
    "                    tjson=tweet._json\n",
    "                    # filter duplicate tweets\n",
    "                    if tjson['id_str'] not in tweet_ids:\n",
    "                        # add tweet to text file\n",
    "                        fp.write(json.dumps(tjson)+\"\\n\")\n",
    "                        # add tweet id to tweet_ids\n",
    "                        tweet_ids.append(tjson['id_str'])\n",
    "            max_id=new_tweets[-1].id\n",
    "            tweet_count+=len(new_tweets)\n",
    "            print(f\"Number of tweets: {tweet_count}\")\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"some error: \" + str(e))\n",
    "            break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read keywords\n",
    "\n",
    "input_keywords=\"input_keywords.txt\"\n",
    "# read keyswords in\n",
    "with open(input_keywords) as fp2:\n",
    "    keywords = fp2.readlines()\n",
    "    \n",
    "# remove newlines\n",
    "for i in range(len(keywords)):\n",
    "    keywords[i] = keywords[i].replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 100\n",
      "Number of tweets: 100\n",
      "Number of tweets: 100\n",
      "Number of tweets: 100\n"
     ]
    }
   ],
   "source": [
    "### Crawl tweets from most recent\n",
    "\n",
    "# call crawel tweets on each keyword\n",
    "for k in keywords:\n",
    "    crawl_tweet(file_name='tweets.json', search_key=k, max_tweet=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 98\n",
      "Number of tweets: 100\n",
      "Number of tweets: 100\n",
      "Number of tweets: 97\n"
     ]
    }
   ],
   "source": [
    "### crawl tweets from oldest stored\n",
    "\n",
    "# call get_ids on starting file\n",
    "tweet_ids = get_ids('tweets.json')\n",
    "last = min(tweet_ids)\n",
    "# call crawel tweets on each keyword\n",
    "for k in keywords:\n",
    "    crawl_tweet('tweets.json', k, 100/len(keywords), last)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
